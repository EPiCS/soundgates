\chapter{Introduction}
	\label{chapter:Introduction}
	\section{About this document}
	
"`Soundgates â€“ Interactive Music Synthesis on FPGAs"' is a project initiated by the Computer Engineering Group of the University of Paderborn, aiming to synthesise music on modern FPGAs. 
Furthermore, users should be able to interact with the system in a playful way by manipulating the synthesised music with advanced sensors such as the acceleration sensor in modern smartphones.


This document is the project plan of our project group.
Not only should this document serve as a basis for the later evaluation and grading, 
but also as a reference for our group during the main working phase running from ??.2013 to ??.2014.

This first chapter introduces some concepts and system we plan to use to achieve the goals outlined in Chapter \ref{chapter:Goals}. 
Chapter \ref{chapter:RelatedWork} covers systems similar to what we are going to create, escpecially the software called MAX.	
Finally Chapters \ref{chapter:Organization} and \ref{chapter:Workplan} describe how the group will organize itself and the groups' milestones.

	\section{Definitions}
	 The following table displays some definitions, that will be used throughout this document.\\
	
	 \begin{tabular}[h]{|c|p{9.75cm}|}
	  \hline
	  Term & Definition \\
	  \hline
	  \hline
	  Composite Component & Definition \\\hline
	  Component & A basic building block to generate music ??Haben wir uns hier auf Block als Bezeichnung geeinight? Component eher im Sinne von Softwarekomponente\\\hline
	  Editor & The Editor is used to create a patch out of components to generate synthesizable code which can be put on a FPGA \\\hline
	  FPGA & Field Programmable Gate Array \\\hline
	  Patch & The entire system which consists of Components and Composite Components. A set of single Components can build a new Component \\\hline
	  Port & The interface from one Component to another one \\\hline
	  Simulation & The developed patch is played through the PC speakers \\\hline
	 \end{tabular}
	 

	\section{Generative music}
	

	\subsection{Introduction}

	As pointed out in \cite{Chandra2012}, there are many cultures where musicical experience is defined by people performing and people perceiving music. 
	The only way to slightly exert influence on the performer's music is by cheering, shouting, etc. on a concert. 
	The gap between performer and perceiver reaches its peak in the context of recorded music like CDs or MP3 files, where is no chance of manipulate the music. 
	Actually, there is a rising trend for interacticing with music on your own or with a familiar social partner, like in the video game "`Guitar Hero"' \cite{Chandra2012, Planck2009}. 
	Even without any musical knowledges or talent, it is more and more possible to interact, manipulate or even create music.
	Generative music combines the opportunities of making music without having knowledges of how to play an instrument and explicitly exert influence on music you hear.
	An early an popular example for generative music was Mozart's musical dice game. Given a set of small sections of music, it was randomly chosen which part was played next.


	\subsection{Approaches} 

	Genarative music (or algorithmic music) can be devided into the following subcategories \cite{Wooller2005}.

	\subsubsection{Linguistic/Structural}

	Already existing songs are scanned in the first place and afterwards recomposed randomly. Therefore, a grammer is created where the production rules are made of sequent pairs of notes. The generation of the new song is achieved by executing this production rules randomly.

	\subsubsection{Creative/Procedural}

	There are different processes/procedures which play predefined music. This processes have an arbitrary order and starting time. A popular is Mozart's musical dice game. 

	\subsubsection{Biological Emergent}

	Music is generated by biological phenomenons, i.\,e. by differing parameters within an ecology, for example wind-chimes.

	\subsubsection{Interactive/Behavioural}

	The interactive/behavioural generative music approach results from processes without discernable musical inputs. It is a question of synthesized music and recorded and filtered samples at the most. The music generation is fully controlled by user input and interaction.
	
	This is the initial point on what this project is based: synthesizing music along with a highly interactive interface to exert influence on the music.

	\section{Possible User Interactions}
	In order to make the system interactive there need to be one or more interfaces, over which the user can take influence on the music. This could happen via midi keyboard or other electronic instruments. But with the purpose of providing an easy, playful access to music generation without knowledges of an instruments, we are focussing on advanced sensors. Sensors like Microsoft's Kinect systems can easily track e.\,g. the user's hand. An intuitive way of changing system's parameters is to measure and transmit the height of the hand. Other possible sensors can be found in nearly every modern smartphone, e.\,g. the acceleration sensors.
	
			
				
	\section{Introduction to sound synthesis}
		Sound synthesis is the artificial creation of sound. 
		Cleverly built synthesizers can mimic many real instruments 
		and of course are able to create sounds with no real world counterpart.
		
		The first synthesizers consisted of seperate analog electronic blocks.
		A user could connect these blocks with cables in many different ways to generate, modify and filter a signal, depending on the order and fashion of connected blocks.
		As with many other systems, digital components replaced the analog ones over time.
		The general approach, however, remained the same. 
		Users do not necessarily plug in cables anymore but conceptually one still connects separate building blocks to create the sound that he wants.
		
		The most basic digital synthesizer would be the one shown in Figure \ref{fig:sound_generation}. 
		In this example an oscillator generates a waveform, for example a basic sine wave.
		Sampled values of the waveform are fed into an Digital-Analog-Converter (DAC) which converts them to an analog signal which can then be played on a speaker.
		While it is not very pleasing to listen to an unmodified sine wave, it is already some artifical and therefore synthesized sound.
		
		\subsection{Frequency and notes}
		The previous example only used a single sine wave. 
		With a small addition this setup can already be used to play notes.
		The only thing that needs to be added would be a way to control the frequency of the sine wave since the value of a note is determined only by the frequency of the sound wave. 
		An A4 for example corresponds to exactly 440 Hz.
		This is true regardless of the actual shape of the waveform, be it a sine wave, a square wave or something completely different. 

		Thinking of different instruments this circumstance quickly becomes obvious. 
		While both a violin and a piano can play the same specific note it will sound differently on both instruments.
		This is due to the fact, that the sound waves those instruments create have a different shape.
		
		\subsection{Waveform generation}
		Creating a specific base shape of our sound can be done in several ways, each with it's advantages and disadvantages and a spectrum of sounds that can be created.
		Of course, many of these approaches are not mutually exclusive but can be combined with each other.
		\subsubsection{Additive synthesis}
			In additive synthesis one uses multiple waveform generators, usually sine waves, adds up their outputs and normalizes the result.
			For example, adding a sine wave with frequency $2F$ to a sine wave with frequency $F$ would result in a different sound color but still with an overall frequency of $F$ and therefore the same note.
			
			A sound designer has very tight control over the produced sound with additive synthesis.
			According to the fourier theorem it is possible to approximate every periodic function by the combination of sines. 
			Therefore, a real world sound could be analyzed and rebuilt this way.
			However this approach may need a lot of separate oscillators and might therefore become unfeasible to handle for very complex sounds with a broad frequency spectrum.
		\subsubsection{Subtractive synthesis}
			A subtractive synthesis starts with waveforms like a sawtooth, square, triangle or even periodic noise.
			Contrary to a bare sine wave, these patterns have a wide frequency spectrum. 
			By feeding these waves into diferent filters (section \ref{subsec:filters}) the broad spectrum can then be reduced again. 
			
			Compared to additive synthesis, it is cheaper to create rich sounds with subtractive synthesis but it sacrifices the very tight control over every single frequency.
		\subsubsection{Frequency/amplitude modulation}
			Another possibility is to modify the frequency or amplitude of one waveform by the value of another.
			This can create relatively rich results with relatively little effort.
			However, this approach usually creates hearable vibrato or tremolo effects.
		\subsubsection{Sampling}
			Any result of the aforementioned approaches, as well as real sound can of course be saved and reused.
			It is relatively cheap and efficient to store small sound samples. 
			Instead of calculating the values of a sine wave, they could just be taken from a lookup table.
			Or instead of trying to synthesize a violin, a real one could be recorded and sampled later for further use.
			
			Such samples are very inflexibile of course.
			It is possible to modify the frequency by changing the playback speed and it is possible to	process them further, but they cannot serve as the only basis for a synthesizer that should be able to create arbitrary sound.			
		\subsection{Filters and further modulation}
			\label{subsec:filters}
			- High pass / low pass
			- ADSR
			- echo /chor / delay	
	
	  	\begin{figure}[!h]
		\centering
			\includegraphics[width=0.90\textwidth]{images/sound_generation.pdf}
		\caption{Easiest way to generate sound on a digital system}
		\label{fig:sound_generation}
	\end{figure}
		
		
		
		
		%- Artifical generation of sound
	  %- Generation of basic waveforms
	    %- More complex and rich patterns by methods like additive/subtractive/... synth 
	  %- Further addition of filters etc
	  %
	  %- Originated from analog synthesizers, nowadays mostly digital. Software for general purpose PCs exist
	  %
	  %- Building patches: job of a sound designer, rather than a musician
			
			
			
			%\section{Project outline}  -- das steht in goals oder?
	  %- Creating an editor for synthesizers. 
	  %- Components of the synthesizer implemented in hardware (and software for simulation purpose)
	  %- Implementation of generative music concepts
	  %------- Warum wollen wir das eigentlich tun? Was ist die Motivation dahinter (zumal es das aus Oslo ja aschon in Software gibt)
		
	\section{Employed systems}
	  \subsection{Xilinx Platform Studio}
	    The overall hardware system is implemented in VHDL, using the Xilinx Platform Studio (XPS), which offers the MicroBlaze Softcore Processor.
	  \subsection{ReconOS}
		ReconOS is an Operating System which can be run on a softcore CPU on a FPGA. Through it's support for the Linux kernel it is possible to write applications which consist of Hard- and Software threads. Therefore the software threads can be used to communicate via sockets with sensors and hand over these values to the hardware threads, which are responsible for generating sounds.
	  \subsection{Eclipse/GMF}
		We avoid building a new graphical editor from ground up since this is an error prone process. Instead we rely on the Model Driven Software Development process for graphical editors which is provided by the GMF framework. Therefore we can generate an editor by specifing the Meta-Model. The graphical surface with our components are connections are the result. Additionally it is neccessary to provide functions for every component, so it will be possible to simulate the generated output.
      